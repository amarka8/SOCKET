2026-01-09 10:52:49,132 | INFO : Experiment longbench_LongAlpaca-12k_Llama-3.1-8B-Instruct_SmallWorld (SEED=42) started at 2026-01-09 10:52:49.074509-06:00 with the following config: 
2026-01-09 10:52:49,133 | INFO : {
    "pipeline_params": {
        "method": "smallworld",
        "kv_cache_method": "stream-llm",
        "model_name": "meta-llama/Llama-3.1-8B-Instruct",
        "save_path": "/scratch/sj157/checkpoints/Llama-3.2-1B-Instruct-selector",
        "load_model_path": null,
        "train_mode": "inference_only",
        "n_init": 0.2,
        "n_local": 0.3,
        "only_eval": true,
        "resume": 0,
        "bf16": false,
        "batch_size_training": 1,
        "reset_optimizer": true,
        "lr": 0.0001,
        "weight_decay": 0.01,
        "gradient_accumulation_steps": 1,
        "num_epochs": 2,
        "eval_period": 1,
        "n_new_tokens": 64,
        "max_model_len": 128000,
        "random_walk_hadamard_dim": 128,
        "truncation_mode": "middle"
    },
    "eval_params": {
        "dataset": "qasper",
        "dataset_path": "dataset/longbench",
        "instruction": "You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nArticle: {context}\n\n Answer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: {input}\n\nAnswer:",
        "instruction_position": "prefix",
        "max_new_tokens": 128,
        "eval_metrics": [
            "qa_f1_score"
        ]
    },
    "train_params": {
        "dataset": "LongAlpaca-12k",
        "dataset_path": "Yukang/LongAlpaca-12k",
        "save_dir": "processed_dataset/LongAlpaca-12k-longctx/",
        "instruction": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:",
        "instruction_position": "prefix",
        "max_new_tokens": 128,
        "eval_metrics": [
            "qa_f1_score"
        ]
    },
    "eval_results": {},
    "management": {
        "exp_desc": "longbench_LongAlpaca-12k_Llama-3.1-8B-Instruct_SmallWorld",
        "pipeline_config_dir": "config/pipeline_config/SmallWorld/Llama-3.1-8B-Instruct/Llama-3.1-8B-Instruct-inference-32hadamard.json",
        "eval_config_dir": "config/eval_config/longbench/qasper.json",
        "output_folder_dir": "/scratch/sj157/Small_World_Attention/scripts/small_world_inference/Llama-3.1-8B-Instruct/longbench/SmallWorld/Llama-3.1-8B-Instruct/LongAlpaca-12k/",
        "job_post_via": "slurm_sbatch",
        "slurm_info": {
            "slurm_job_id": "75961",
            "slurm_job_name": "trial",
            "slurm_out_file_dir": "/scratch/sj157/slurm-75961.out"
        },
        "sub_dir": {
            "input_config": "input_config/",
            "raw_results": "raw_results.json",
            "result_vis": "result_vis.png",
            "output_config": "output_config.json"
        }
    }
}
2026-01-09 11:00:56,050 | INFO : raw_results file saved to /scratch/sj157/Small_World_Attention/scripts/small_world_inference/Llama-3.1-8B-Instruct/longbench/SmallWorld/Llama-3.1-8B-Instruct/LongAlpaca-12k/raw_results.json.
2026-01-09 11:00:56,052 | INFO : Experiments concluded, below is the raw_results: 
2026-01-09 11:00:56,052 | INFO : {
    "total_score": 51.02320098876953,
    "total": 200.0
}
2026-01-09 11:00:56,052 | INFO : ##### And below is the processed_results: #####
2026-01-09 11:00:56,053 | INFO : {
    "dataset": "qasper",
    "score": 25.5116024017334,
    "outputs": [
        "The ground truth for fake news is established by manually verifying the accuracy of the information in the tweets, although the article does not provide further details on the verification process.",
        "An extension of the VLAD approach that was originally proposed for face recognition, used to aggregate frame-level features into a single utterance level feature for language identification.",
        "68.4%",
        "Context-dependent characteristics, previous threads, user reporting processes, artificial intelligence, human reviewers, and othering language are proposed as additional features and context.",
        "Fox News, CNN, BBC News, The New York Times, The Guardian, and The Washington Post were not mentioned in the article. However, the article does mention that the authors selected Facebook pages based on their performance on development data, and label distribution, but it does not provide a list of specific pages.",
        "yes",
        "Extrinsic evaluation using a gold-standard corpus and a baseline method.",
        "The article mentions that the proposed approach is evaluated on three single-document news summarization datasets, which are representative of different writing conventions and summary styles.",
        "The approach is compared to other WSD approaches employing word embeddings, and it is shown to outperform them in certain tasks, such as handling polysemous words and entailment relations.",
        "Their ensemble method works by selecting the best performing models according to validation performance and gradually adding them to the ensemble.",
        "The Friends dataset comes from the scripts of the Friends TV sitcom, and the EmotionPush dataset is made up of Facebook messenger chats.",
        "English.",
        "The English Wiki News Abstract by BIBREF8 and the English Wiki Simple (SW) Articles by BIBREF9 are not used for sentiment analysis, but the Billion Word (BW) is not used for sentiment analysis either. The article does not specify the dataset used for sentiment analysis, but it does mention that the English Wiki News Abstract and the English Wiki Simple (SW) Articles are used for word embeddings. However, it does mention that the corpus used for sentiment analysis is the IMDb.",
        "The proposed system outperforms strong baseline systems, but the specific accuracy is not mentioned in the article.",
        "Yes.",
        "The Switchboard Corpus, the Fisher Corpus, the Switchboard-300, the Fisher-500, the Switchboard-100, the Fisher-100, the Switchboard-50, the Fisher-50, the Switchboard-20, the Fisher-20, the Switchboard-10, the Fisher-10, the Switchboard-5, the Fisher-5, the Switchboard-2, the Fisher-2, the Switchboard-1, the Fisher-1, the Switchboard-0.5, the Fisher-0.5, the Switchboard-0.2, the Fisher-0.2, the",
        "unanswerable",
        "RSM models, SMT models, and various NMT models.",
        "A regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution.",
        "1. We propose UTCNN, a neural network for text in modern social media channels as well as legacy social media, forums, and message boards \u2014 anywhere that reveals users, their tastes, as well as their replies to posts. \n2. When classifying social media post stances, we leverage users, including authors and likers. User embeddings can be generated even for users who have never posted anything. \n3. We incorporate a topic model to automatically assign topics to each post in a single topic dataset. \n4. We show that overall, the proposed method achieves the highest performance in all instances, and that all of the information",
        "They greatly improved the performance on the fine-grained sentiment classification task.",
        "It eases interpretability and leads to slight accuracy gains.",
        "The baseline was a context-agnostic MT system.",
        "L2 distance, L1 distance, and cosine similarity are used for evaluation, but the article does not specify these metrics. However, the article does mention that the models are evaluated on two zero-shot cross-lingual transfer tasks, namely natural language inference and universal dependency parsing, and that the performance of the models is compared to mBERT.",
        "MT.",
        "Emoticons, emojis, and other non-linguistic modalities are not explicitly mentioned, but emoticons and emojis are not obtained. However, emoticons and emojis are not explicitly mentioned in the article.",
        "A single-layer architecture is not specified, but the encoder is described as a character-based encoder-decoder model with a wide context window.",
        "Yes.",
        "unanswerable",
        "22,000 blog users.",
        "BIBREF0 is unanswerable, BIBREF1 is perplexity, BIBREF2 is user-ranking, BIBREF3 and BIBREF4 are unanswerable, BIBREF5 is unanswerable, and the article also mentions quantitative and qualitative analysis, and new evaluation strategies for generation quality in instructional texts, centering on quantitative measures of coherence.",
        "They create labels for their dataset using a framework to construct a simulated human-human dialogue dataset.",
        "unanswerable",
        "Machine translation and other complex tasks.",
        "The article does not provide a specific improvement in performance for Estonian in the NER task.",
        "The authors have a diversity of disciplinary backgrounds and research practices.",
        "no \n\nThe paper uses LDA to obtain topic probabilities, but the features (GOSS and LOSS) are then used in a supervised manner to discriminate between spammers and legitimate users.",
        "The Nguni languages of South Africa, including Zulu, Xhosa, and Swati, are similar to each other, and the Bantu languages of South Africa, including Sotho and Venda, are also similar to each other.",
        "conventional fully-connected feed-forward deep neural network, conventional fully-connected feed-forward deep neural network, conventional fully-connected feed-forward deep neural network, conventional fully-connected feed-forward deep neural network, conventional fully-connected feed-forward deep neural network, conventional fully-connected feed-forward deep neural network, conventional fully-connected feed-forward deep neural network, conventional fully-connected feed-forward deep neural network, conventional fully-connected feed-forward deep neural network, conventional fully-connected feed-forward deep neural network, conventional fully-connected feed-forward deep neural network, conventional fully-connected feed-forward deep neural network, conventional fully-connected feed-forward deep neural network, conventional fully-connected feed-forward deep neural network, conventional",
        "The Wikipedia dataset is novel to this paper, and the arXiv dataset is provided by BIBREF2 and split into three sub-parts based on subject category, with no specific size mentioned.",
        "A group of 5 people were asked to judge the quality of the translations, and their judgements were used to calculate the BLEU score.",
        "Yes.",
        "The models are evaluated based on the efficiency of typing keywords, accuracy of reconstruction, and interpretability of keywords.",
        "Precision, recall, F1 score, accuracy, and ADWS kernel are mentioned as evaluation metrics for classification tasks.",
        "The source domain is an existing domain with sufficient labeled data, and the target domain is a new domain with very few or no labeled data.",
        "They compare with LSTMs and other previous RNN models, including recent work in language modeling.",
        "Embeddings, recurrent neural networks, convolutional neural networks, long short-term memory (LSTM), and transformers are included in NeuronBlocks.",
        "The datasets used were the International Phonetic Alphabet (IPA) and Phoible, URIEL, and WALS.",
        "unanswerable",
        "English, Spanish, and Basque.",
        "Named tasks include generalizing to out-of-vocabulary words, sequences at test time, and tracking infectious diseases.",
        "Yes.",
        "yes, it was evaluated against traditional modular systems and more recent end-to-end task-oriented dialogue systems.",
        "They obtain psychological dimensions of people by analyzing their blog posts and categorizing them into semantic and semantic categories.",
        "claim, grounds, warrant, backing, qualifier, and rebuttal.",
        "n-grams",
        "1,014 posts from 1,000 users.",
        "English, Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese, German, Italian, Russian, Spanish, and Arabic.",
        "The two datasets the model is applied to are conversations from online platforms, specifically the \"Wikipedia Talk Pages\" and \"Reddit\".",
        "unanswerable",
        "The quality of the data is empirically evaluated through several methods, including augmenting transcripts with English translations, creating a corpus from LibriVox audiobooks, and using a multilingual speech corpus based on Bible readings.",
        "They combine audio and text sequences in their RNN by using high-level text transcription, as well as low-level audio signals, to utilize the information contained within low-resource datasets to a greater degree.",
        "Our model achieved substantial gains in all tasks, and new SOTA results, via back-translation of simplified sentences into the ordinary sentences, and treating this synthetic data as additional training data.",
        "7 humans evaluated the results.",
        "A tweet is considered to have gone viral if it has been studied as a proxy for users' exposure to the tweet.",
        "unanswerable",
        "crowdsourcing using an Android application.",
        "Logistic Regression, Recurrent Neural Networks (RNNs), Long Short Term Memory cells (LSTMs), and Convolutional Neural Networks (CNNs) are used for RQE.",
        "The benchmark dataset is the social honeypot built by alluring social spammers' retweet, and its quality is extensively explored in the paper.",
        "An LSTM architecture is not explicitly mentioned, but the decoder is described as an LSTM-like architecture is not mentioned, however, the decoder is described as an LSTM-like architecture is not mentioned, however, the decoder is described as an LSTM-like architecture is not mentioned, however, the decoder is described as an LSTM-like architecture is not mentioned, however, the decoder is described as an LSTM-like architecture is not mentioned, however, the decoder is described as an LSTM-like architecture is not mentioned, however, the decoder is described as an LSTM-like architecture is not mentioned, however, the decoder is described as an LSTM-like architecture is not mentioned, however",
        "no",
        "The best performing model among author's submissions is the model that used features, ensemble schemes, multi-tasking, and multi-granularity architectures, which achieved 3rd and 4th ranks in FLC and SLC tasks respectively, but the exact performance metrics are not specified in the article.",
        "The baseline was a model trained on a resource-rich language pair like French English, fine-tuned for a low-resource language pair like Uzbek English.",
        "0.6103 for Factoid Question Answering task is not their highest recall score, their highest recall score is 0.6103 is for MRR score for Factoid Question Answering task, their highest recall score is 0.6103 is not for recall score, their highest recall score is 0.6103 is not for recall score, their highest recall score is 0.6103 is not for recall score, their highest recall score is 0.6103 is not for recall score, their highest recall score is 0.6103 is not for recall score, their highest recall score is 0",
        "Word embedding techniques such as word2vec are explored in the paper.",
        "They use a model to generate contextual representations.",
        "unanswerable",
        "Seven experts were used for annotation, including Elias Wright, Gian Mascioli, Kiara Pillay, Harrison Kay, Eliel Talo, Alexander Fagella, and N. Cameron Russell.",
        "The CNN-RNN model is used for painting embedding, and the sequence-to-sequence model with global attention is used for language style transfer.",
        "The transformer layer works better, as it achieves state-of-the-art performance in several language understanding tasks.",
        "yes",
        "personal attacks on race, religion, and gender.",
        "They propose extended middle context, a new context representation for CNNs, which uses all parts of the sentence and pays special attention to the middle part.",
        "There are four main categories of entities: \"PERSON\", \"LOCATION\", \"ORGANIZATION\" and others, and four specific South Asian languages: Hindi, Indonesian, Kannada, Malayalam, Tamil, Telugu, and Nepali.",
        "The resulting annotated data is of higher quality when experts annotate difficult examples rather than instances selected at i.i.d. random.",
        "65% of analyzed corpora are male.",
        "The Multi30K dataset.",
        "BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF13, BIBREF2, BIBREF15, BIBREF17, BIBREF18, BIBREF21, BIBREF22, BIBREF23.",
        "Logistic regression and deep neural networks are used, but the article specifically mentions that statistical machine learning models are used for event detection.",
        "NLTK, Stanford NLP, CogComp-NLP, and TensiStrength are used.",
        "The SQuAD dataset and the SQuAD reading comprehension materials.",
        "Various approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries.",
        "Yes.",
        "The authors used the following datasets for evaluation: topic identification of spoken conversations and call center customer satisfaction prediction, specifically using transcripts of calls that sometimes exceed 5000 words.",
        "The IMDb dataset is not mentioned, but the article does mention that the quasi-recurrent neural networks (QRNNs) outperform strong LSTM baselines on document-level sentiment classification.",
        "Yes.",
        "unanswerable",
        "The invertibility condition is that the non-linear projector is chosen to be an invertible neural network, and the model is parameterized in terms of the projection's inverse.",
        "The proposed qualitative annotation schema is richly annotated with categories such as linguistic complexity, reasoning, and potential external knowledge required to obtain the expected answer.",
        "The sizes of the two datasets are not explicitly mentioned in the article. However, the article does mention that the Wikipedia data used for simplified sentences is utilized, and it is implied that the datasets are large enough to be used for training and testing purposes.",
        "Vanilla Connectionist Temporal Classification (CTC) model, Tandem Connectionist Temporal Classification (CTC) model, and a model that uses a concatenation of an ASR encoder and an MT encoder.",
        "English.",
        "The models used in the experiment are Support Vector Machines (SVMs) and neural networks, specifically a CNN-based sentence classifier.",
        "unanswerable",
        "GloVe and word2vec were not mentioned, but the article does mention that the authors used GloVe and word2vec in their previous work, however, in this article, the authors used GloVe and word2vec were not used, but the authors used GloVe and word2vec were not used, but the authors used GloVe and word2vec were not used, but the authors used GloVe and word2vec were not used, but the authors used GloVe and word2vec were not used, but the authors used GloVe and word2vec were not used, but the authors used GloVe and word",
        "All three personalized models (Prior Tech, Prior Name, Prior Recipe) generated high-quality and specific recipes that aligned with historical user preferences, as shown qualitatively and quantitatively.",
        "The combination of rewards for reinforcement learning is the irony accuracy reward, sentiment preservation reward, and content preservation reward.",
        "The authors demonstrate that their model may not perform well in the absence of non-parallel datasets.",
        "The existing benchmarks they compared to were the Hashtag Emotion Lexicon and the work of go2009twitter, hallsmarmulti, mohammad2015using, and others.",
        "The article does not provide a comprehensive list of their distribution results, but it mentions that they found significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users between tweets containing fake news and tweets not containing them.",
        "The dataset of hashtags is sourced from Twitter and Instagram, and is associated with tweets annotated in a multi-step process.",
        "unanswerable",
        "A compact representation of the context of the corresponding text.",
        "The baseline model used is a supervised perceptron model for finding the most representative paragraph that is the least similar to paragraphs in other sections.",
        "unanswerable",
        "unanswerable",
        "The dataset used was the tweets associated with a five-point scale ranging from VeryNegative to VeryPositive, and the authors also used the data from review sites like Amazon and TripAdvisor.",
        "They use small BERT, specifically the pre-trained BERT model.",
        "Yes, the article mentions that the authors carefully constructed baselines and closely inspected the data to ensure probe quality.",
        "Yes, the images are from the ShapeWorld framework.",
        "Our models achieved state-of-the-art results on emotion detection, outperforming previous methods.",
        "The tagging scheme employed is a two-group categorization of English puns, namely heterographic puns and homographic puns.",
        "unanswerable",
        "The model is considered robust if it is insensitive to the prior knowledge supplied to the learning model.",
        "InferSent, Universal Sentence Encoder, GloVe embeddings, and SentEval.",
        "The method improves F1 for English datasets by 5.4% and for Chinese datasets by 4.5%.",
        "Quora-style question answering and passage retrieval, specifically on tasks such as semantic similarity, passage retrieval, and classification.",
        "They compared against several baselines, including recursive neural networks (RvNN), convolutional neural networks (CNN), self-attention based models, latent tree-structured models, and typical LSTM models.",
        "Relation detection.",
        "A simple encoder-decoder model and two other models (Prior Tech and Prior Recipe) are not baseline models, but rather the Prior Tech and Prior Recipe models are the personalized models being compared to the baseline model. The baseline model is actually the Prior Recipe model without personalization.",
        "Browser-based methods such as manually browsing through the Flickr30K dataset, and more systematic methods like using a concrete example from the dataset.",
        "English, French, and other languages, but primarily English.",
        "They experimented with stacked RNNs, LSTMs, GRUs, and their variants with residual connections, shortcut connections, and cell states.",
        "Yes.",
        "The authors experimented with an ILP-based summarization technique and compared it with manual summaries, but they also mentioned that they used other summarization algorithms in the past, however, they did not specify which ones.",
        "The previous state of the art for instructor intervention in MOOC forums was proposed by earlier studies, which either modelled the entire context or required the context size to be specified explicitly.",
        "The READOUT pooling function is the least impactful.",
        "The corpus used for the task is a digitized text resource, specifically the \"same data set\" which is not specified in the article.",
        "Kannada, Hindi, Marathi, Bengali, Telugu, Tamil, and Gujarati.",
        "The model performance on target language reading comprehension is comparable to human-level performance on SQuAD, one of the most widely used RC benchmarks.",
        "ALOHA, the proposed model, outperforms baselines.",
        "ARAML performs better than several state-of-the-art GAN baselines with lower training variance.",
        "The authors present evidence that their model can detect some biases in data annotation and collection by examining the results, which show the model's ability to identify biases in the process of collecting or annotating datasets.",
        "Yes.",
        "The dataset size is not explicitly mentioned in the article.",
        "Using dice loss, the proposed training objective leads to a 0.5% improvement in F1 score for paraphrase identification.",
        "The article does not explicitly mention the datasets used, but it mentions that the results in Tables TABREF23, TABREF24, and TABREF25 are based on a dataset, and that the results in Tables TABREF23 and TABREF24 are compared to the results in Table TABREF25.",
        "unanswerable",
        "Pointer Baselines include a summarization model, a template-based model, and a keyword-based model.",
        "Traditional machine learning models are not used, but rather, the article mentions the use of artificial intelligence and human reviewers on the dataset.",
        "A bi-directional transformer and a bi-directional transformer with a pre-trained language model.",
        "The weights are dynamically adjusted in proportion to (1-p), where p is the probability of a training example, and this weight changes as training proceeds.",
        "Both proposed strategies, policy chaining and Go-Explore, surpass bottlenecks in text-adventure games, with the policy chaining method showing significant improvement in exploration efficiency.",
        "A Bayesian model for each language.",
        "Annotations or notes are used to identify non-standard pronunciation.",
        "A semi-character architecture is a type of word recognition model that builds upon RNN-based semi-character word recognition models.",
        "Bulgarian, Dutch, English, French, German, Italian, Portuguese, Spanish, and Swedish are explored, but the article does not specify the exact languages. However, it does mention that the research is applicable to any language, provided adequate training data is available.",
        "NCEL outperforms the state-of-the-art collective methods across five different datasets.",
        "yes",
        "The baseline used was the system by Felice2016, which showed that additional training data has the biggest impact on improving performance.",
        "The 2010 i2b2/VA BIBREF0 and 2013 ShARe/CLEF Task 1 challenges.",
        "It allows the refine decoder to predict the refined word one-by-one using a pre-trained language model.",
        "The article does not explicitly mention a specific dataset used in the research. However, it mentions that the researchers are focusing on leveraging unsupervised representation learning methods based on neural networks to solve the problem of modeling tweets, which implies that the dataset is likely Twitter data.",
        "TF-IDF features and existing classification models.",
        "The dataset is annotated with labels for eating disorders, influenza, prescription drug and smoking behaviors, mental health disorders, and major depressive disorder.",
        "The eight biomedical NER tasks used for evaluation are not explicitly listed in the article, but it is mentioned that they improved over general-domain BERT on eight out of eight biomedical NER tasks.",
        "The training data was translated using machine translation.",
        "They used a content-based classifier and an ensemble that incorporates several metadata classifiers.",
        "A feature-based system for discriminating between propagandist and non-propagandist articles.",
        "They compare with systems developed separately for pun detection and pun location, as well as a system that makes a binary prediction on whether a sentence contains a pun or not.",
        "The political bias of different sources is included in the model by considering a set of basic features which can be qualitatively interpreted w.r.t to the social behavior of users sharing credible vs non-credible information, and by using off-the-shelf machine learning classifiers to accurately classify news articles leveraging Twitter diffusion networks.",
        "The ancient Chinese dataset comes from a large translation parallel corpus created using the proposed method, which contains 1.24M bilingual sentence pairs.",
        "English.",
        "standard Chinese PTB.",
        "unanswerable",
        "The dataset used in this paper includes Flickr tags, numerical environmental features, and categorical information from various sources, including structured environmental datasets and land cover types.",
        "NUBes and MEDDOCAN: Medical Document Anonymization shared task dataset.",
        "Unigrams and Pragmatic features.",
        "Avg. precision, recall, F1 score, and accuracy are used to evaluate the performance of the proposed lifelong interactive learning and inference (LiLi) approach.",
        "Yes.",
        "Galatasaray and Fenerbah\u00e7e are not mentioned in the article. The article mentions that the targets are two popular sports clubs in Turkey, but it does not specify which clubs they are.",
        "The authors conduct experiments on irony generation, including building a large-scale dataset from Twitter, training a model to generate ironic sentences, and comparing its performance with other generative models.",
        "Gaussian-masked directional multi-head attention works by using a variant of self-attention that replaces the standard self-attention to improve the ability of capturing the localness and directional information of self-attention based encoder.",
        "Facebook, Twitter, and Yelp reviews, as well as general social media.",
        "The network's baseline features are the features extracted using pre-trained sentiment, emotion, and personality models.",
        "The number of clusters and the dimensionality of the word embeddings were varied in the experiments.",
        "Their system ranked second (EI-Reg), second (EI-Oc), and fourth (V-Reg) in the Spanish subtasks.",
        "The corpus contains 10,000 clinical case reports.",
        "unanswerable",
        "Text categorization and sentiment classification.",
        "Previous methods such as customized rule-based pattern matching, a combination of rule-based and machine learning approaches, CNN variants, and LSTM variants are compared to their model.",
        "The training sets of these versions of ELMo are significantly larger, with the previous ones being based on large monolingual corpora.",
        "694 sentences in the OurNepali dataset and 1,000 sentences in the dataset used for training the model.",
        "MLP, Eusboost, Mwmote, and traditional classifiers.",
        "Yes.",
        "Yes.",
        "0.6103 in one of the test batches for Factoid Question Answering task.",
        "The Penn Treebank and the Penn Treebank is used for POS induction and unsupervised dependency parsing without gold POS tags.",
        "They mention that engineers often face challenges when applying DNN models to NLP tasks, which hinders their productivity.",
        "SimpleQuestions and WebQuestions."
    ]
}
2026-01-09 11:00:56,059 | INFO : output_config file saved to /scratch/sj157/Small_World_Attention/scripts/small_world_inference/Llama-3.1-8B-Instruct/longbench/SmallWorld/Llama-3.1-8B-Instruct/LongAlpaca-12k/output_config.json.
2026-01-09 11:00:56,061 | INFO : Experiment longbench_LongAlpaca-12k_Llama-3.1-8B-Instruct_SmallWorld ended at 2026-01-09 11:00:56.053218-06:00. Duration: 0:08:06.978709
