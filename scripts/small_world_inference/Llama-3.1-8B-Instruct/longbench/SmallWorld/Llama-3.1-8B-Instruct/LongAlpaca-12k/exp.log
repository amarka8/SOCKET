2026-01-11 13:43:17,572 | INFO : Experiment longbench_LongAlpaca-12k_Llama-3.1-8B-Instruct_SmallWorld (SEED=42) started at 2026-01-11 13:43:17.538658-06:00 with the following config: 
2026-01-11 13:43:17,572 | INFO : {
    "pipeline_params": {
        "method": "smallworld",
        "kv_cache_method": "stream-llm",
        "model_name": "meta-llama/Llama-3.1-8B-Instruct",
        "save_path": "/scratch/sj157/checkpoints/Llama-3.2-1B-Instruct-selector",
        "load_model_path": null,
        "train_mode": "inference_only",
        "n_init": 0.2,
        "n_local": 0.3,
        "only_eval": true,
        "resume": 0,
        "bf16": false,
        "batch_size_training": 1,
        "reset_optimizer": true,
        "lr": 0.0001,
        "weight_decay": 0.01,
        "gradient_accumulation_steps": 1,
        "num_epochs": 2,
        "eval_period": 1,
        "n_new_tokens": 64,
        "max_model_len": 128000,
        "random_walk_hadamard_dim": 128,
        "truncation_mode": "middle"
    },
    "eval_params": {
        "dataset": "qasper",
        "dataset_path": "dataset/longbench",
        "instruction": "You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nArticle: {context}\n\n Answer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: {input}\n\nAnswer:",
        "instruction_position": "prefix",
        "max_new_tokens": 128,
        "eval_metrics": [
            "qa_f1_score"
        ]
    },
    "train_params": {
        "dataset": "LongAlpaca-12k",
        "dataset_path": "Yukang/LongAlpaca-12k",
        "save_dir": "processed_dataset/LongAlpaca-12k-longctx/",
        "instruction": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:",
        "instruction_position": "prefix",
        "max_new_tokens": 128,
        "eval_metrics": [
            "qa_f1_score"
        ]
    },
    "eval_results": {},
    "management": {
        "exp_desc": "longbench_LongAlpaca-12k_Llama-3.1-8B-Instruct_SmallWorld",
        "pipeline_config_dir": "config/pipeline_config/SmallWorld/Llama-3.1-8B-Instruct/Llama-3.1-8B-Instruct-inference-32hadamard.json",
        "eval_config_dir": "config/eval_config/longbench/qasper.json",
        "output_folder_dir": "/scratch/sj157/Small_World_Attention/scripts/small_world_inference/Llama-3.1-8B-Instruct/longbench/SmallWorld/Llama-3.1-8B-Instruct/LongAlpaca-12k/",
        "job_post_via": "slurm_sbatch",
        "slurm_info": {
            "slurm_job_id": "76834",
            "slurm_job_name": "trial",
            "slurm_out_file_dir": "/scratch/sj157/slurm-76834.out"
        },
        "sub_dir": {
            "input_config": "input_config/",
            "raw_results": "raw_results.json",
            "result_vis": "result_vis.png",
            "output_config": "output_config.json"
        }
    }
}
2026-01-11 14:02:14,617 | INFO : raw_results file saved to /scratch/sj157/Small_World_Attention/scripts/small_world_inference/Llama-3.1-8B-Instruct/longbench/SmallWorld/Llama-3.1-8B-Instruct/LongAlpaca-12k/raw_results.json.
2026-01-11 14:02:14,619 | INFO : Experiments concluded, below is the raw_results: 
2026-01-11 14:02:14,619 | INFO : {
    "total_score": 86.74613189697266,
    "total": 200.0
}
2026-01-11 14:02:14,619 | INFO : ##### And below is the processed_results: #####
2026-01-11 14:02:14,619 | INFO : {
    "dataset": "qasper",
    "score": 43.37306594848633,
    "outputs": [
        "The ground truth for fake news is established by a single person manually annotating the tweets as containing fake news or not, based on categories described by Rubin et al.",
        "An extension of the NetVLAD approach, which adds ghost clusters to map noisy or irrelevant content into ghost clusters and exclude them during feature aggregation.",
        "68.8% to 71.8%",
        "Context tweets, character-level features, and latent topic clustering (LTC) are proposed as additional features and context.",
        "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.",
        "yes",
        "Extrinsic evaluation is proposed for this task, specifically using manual responsiveness scores, ROUGE-2, and Pyramid scores.",
        "The CNN/DailyMail, New York Times Annotated Corpus (NYT), and XSum datasets are used for evaluation.",
        "The proposed approach, GM_KL, achieves better correlation scores than existing approaches, w2g and w2gm, on various benchmark word similarity and entailment datasets.",
        "Their ensemble method works by selecting the best performing model according to validation performance and adding it to the ensemble if it improves the validation performance, repeating this process until a specified number of models is reached.",
        "The Friends dataset comes from the scripts of the Friends TV sitcom, while the EmotionPush dataset is made up of Facebook Messenger chats.",
        "English.",
        "The IMDb dataset of movie reviews.",
        "The proposed system (ALCrowd) achieves the best performance among all the systems, with +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ, respectively.",
        "Yes.",
        "The Switchboard dataset, the Dialog State Tracking Challenge (DSTC) dataset, and a set of 246,945 documents, including 184,001 Twitter posts and 62,949 news articles, all related to finance.",
        "unanswerable",
        "RNN-based NMT and Transformer-NMT.",
        "A regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution.",
        "1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments.",
        "They improved the state-of-the-art performance by several points, as shown in the table.",
        "It learns to assign exactly zero probabilities to irrelevant words, leading to crisper examples of attention head behavior.",
        "The baseline model was a context-agnostic Transformer model.",
        "Labeled Attachment Scores (LAS) and accuracy for cross-lingual Natural Language Inference (XNLI) test.",
        "MT.",
        "Emoticons, laughter expressions, and hashtags.",
        "A forward LSTM encodes the past context, and a backwards LSTM encodes the future context.",
        "Yes.",
        "unanswerable",
        "22,880 users.",
        "BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, and recipe-level coherence.",
        "They create labels such as \"Open-ended Inquiry\", \"Detailed Inquiry\", \"Multi-Intent Inquiry\", \"Reconfirmation Inquiry\", \"Inquiry with Transitional Clauses\", \"Yes/No Response\", \"Detailed Response\", \"Response with Revision\", and \"Response with Topic Drift\".",
        "unanswerable",
        "Machine translation tasks, specifically four machine translation tasks: IWSLT 2017 German \u2192 English, KFTT Japanese \u2192 English, WMT 2016 Romanian \u2192 English, and WMT 2014 English \u2192 German.",
        "The article does not provide a direct comparison of the improvement in performance for Estonian in the NER task.",
        "The authors have a background in the humanities and social sciences, and are affiliated with The Alan Turing Institute.",
        "no \n\nThe paper uses LDA to compute topic probabilities, but the features (GOSS and LOSS) are then used in a supervised classification approach.",
        "The Nguni languages (zul, xho, nbl, ssw) and the Sotho languages (nso, sot, tsn) are similar to each other.",
        "conventional RNNs, bidirectional LSTM networks, and shallow LSTM networks.",
        "The Wikipedia dataset consists of 29,794 articles, with 5,000 articles from each of the six quality classes.",
        "A group of 50 native speakers who were well-versed in both English and Tamil languages acted as annotators for the evaluation.",
        "Yes.",
        "The models are evaluated based on the retention rate of tokens (efficiency) and the fraction of sentences generated by the model that exactly match the target sentence (accuracy).",
        "Precision, Recall, and F-measure are used for multi-label classification, and accuracy is used for overall classification.",
        "The source domain is an existing domain with sufficient labeled data, and the target domain is a new domain with very few or no labeled data.",
        "They compare with LSTMs and state-of-the-art methods such as RAN, QRNN, and NAS.",
        "Embedding layer, neural network layers (RNN, CNN, Transformer, Highway network, etc.), attention mechanisms (Linear/Bi-linear Attention, Full Attention, Bidirectional attention flow, etc.), and regularization layers (Dropout, Layer Norm, Batch Norm, etc.).",
        "The multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary.",
        "unanswerable",
        "English, Spanish, Finnish, and 14 other languages.",
        "Named Entity Recognition, POS tagging, text classification, and language modeling.",
        "Yes.",
        "yes, the article mentions that the current architecture shows a good trade-off between speed and efficacy with strong and robust performance in empirical evaluations on the response retrieval task using Reddit, OpenSubtitles, and AmazonQA conversational test data.",
        "They use LIWC categories, word frequencies, and the Meaning Extraction Method (MEM) to measure psycholinguistic and semantic properties.",
        "claim, premise, backing, rebuttal, refutation, and non-argumentative text.",
        "n-grams of unspecified order",
        "1,873 conversation threads, roughly 14k tweets.",
        "English, Mandarin Chinese, Russian, French, Spanish, Welsh, Kiswahili, Yue Chinese, Estonian, Finnish, Polish, and Hebrew.",
        "The Wikipedia dataset and the CMV (ChangeMyView) dataset.",
        "unanswerable",
        "The quality of the data is empirically evaluated through various sanity checks, including BLEU score computation, perplexity measurement, and manual inspection of translations.",
        "They combine the audio and text sequences using a dual RNN approach, where the last hidden state of the audio-RNN is concatenated with the final hidden states of the text-RNN.",
        "Our method yields an improvement of 2.11 BLEU, 1.7 FKGL, and 1.07 SARI over the baseline NMT model.",
        "7 annotators evaluated the results.",
        "A tweet is considered to have gone viral if it was retweeted more than 1000 times.",
        "unanswerable",
        "crowdsourcing using an Android application.",
        "Logistic Regression and a deep learning model adapted from Bowman et al. are used for RQE.",
        "The benchmark dataset is the social honeypot dataset created by Lee et al., and its quality is not explicitly stated as high, but it has been extensively explored in the paper.",
        "An LSTM decoder with an attention mechanism.",
        "no",
        "The best performing model among author's submissions is the ensemble of (r4, r7, r12) for SLC task, which achieved a score of 0.673 in F1, and the ensemble of (II and IV) for FLC task, which achieved a score of 0.673 in F1.",
        "The baseline was a weak baseline without using any monolingual data, and a strong baseline established with monolingual data.",
        "0.7033 for List-type question answering task in test batch 4.",
        "Word embedding techniques such as word2vec are explored in the paper.",
        "They use a bilingual dictionary (Google Translate word translation) and a generic re-ordering system that applies to all Indian languages, and also Hindi-tuned rules that improve the generic rules.",
        "unanswerable",
        "Seven experts with legal training were recruited to construct answers to the questions posed by crowdworkers.",
        "The CNN-RNN actor-critic architecture is used for painting embedding, and the sequence-to-sequence model with pre-trained word embeddings is used for language style transfer.",
        "The transformer layer works better, as seen in the ToBERT model outperforming RoBERT on the Fisher and 20newsgroups datasets.",
        "yes",
        "personal attack, racism, and sexism.",
        "They propose extended middle context, which uses all parts of the sentence (relation arguments, left of the arguments, between the arguments, and right of the arguments) and pays special attention to the middle part.",
        "There are four different types of entities: Person (PER), Location (LOC), Organization (ORG), and Miscellaneous (MISC).",
        "The resulting annotated data with expert annotations is of higher quality compared to crowd annotations.",
        "65% of speakers are men, speaking more than 75% of the time.",
        "The English-German dataset.",
        "BIBREF20, BIBREF18, and other recent neural models.",
        "Logistic Regression (LR) and Multilayer Perceptron (MLP) are used.",
        "NLTK, TwitterNLP, CogComp-NLP, spaCy, Stanford CoreNLP, and SentiStrength, among others.",
        "The SQuAD dataset.",
        "Various approaches have been proposed for modelling urban environments, including bag-of-words representations, principal component analysis, stacked autoencoders, and word embedding models.",
        "Yes.",
        "The authors used three datasets for evaluation: CSAT dataset, 20 newsgroups, and Fisher Phase 1 US English corpus.",
        "The IMDb movie review dataset.",
        "Yes.",
        "unanswerable",
        "The invertibility condition is that the neural projector's Jacobian determinant is nonzero and differentiable.",
        "The proposed qualitative annotation schema is a multi-label task that includes categories such as linguistic complexity, required reasoning, and factual correctness, with further subcategories for each dimension.",
        "The WikiSmall dataset has 89,042 training sentence pairs and 100 test pairs, while the WikiLarge dataset has 296,402 training sentence pairs and 2,359 test sentences with 8 reference simplifications.",
        "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pretrain, Triangle+pretrain.",
        "English.",
        "The models used in the experiment are a linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model.",
        "unanswerable",
        "GloVe and Edinburgh embeddings.",
        "All personalized models outperformed the baseline in BPE perplexity, and the Prior Name model performed the best.",
        "The combination of rewards for reinforcement learning is the harmonic mean of the irony reward and the sentiment reward.",
        "The authors demonstrate that their model may not work well with Shakespeare style transfer when the generated English poem does not have similar words in the style transfer dataset.",
        "The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset.",
        "The article presents the results of their distribution analysis in several tables and figures, including the Kolmogorov-Smirnov test, which shows statistically significant differences between viral tweets containing fake news and viral tweets not containing fake news in various dimensions such as the number of hashtags, URLs, and followers.",
        "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset.",
        "unanswerable",
        "A compact, scalable, and meaningful representation of a set of word vectors.",
        "The baseline model used for the article\u2013entity placement task is B1, which uses only the salience-based features by Dunietz and Gillick.",
        "unanswerable",
        "unanswerable",
        "The dataset used was from the SemEval-2016 \"Sentiment Analysis in Twitter\" task.",
        "They use the small BERT model, BERT$_\\mathrm {BASE}$.",
        "Yes, the article mentions that the authors carefully constructed baselines and performed close data inspection to ensure probe quality.",
        "Yes, the images are from the ShapeWorld framework, which is a controlled data generation framework for abstract colored shapes.",
        "Our model's performance on emotion detection was competitive or even state-of-the-art for some emotion labels on existing, standard evaluation datasets.",
        "The tagging scheme employed is { O, P }, and later a new tagging scheme { B, I, E } is proposed to capture the structural constraint that each context contains a maximum of one pun.",
        "unanswerable",
        "The model is considered robust if it can handle bias in prior knowledge and make accurate predictions despite unbalanced labeled features or class distributions.",
        "InferSent, Universal Sentence Encoder, Skip-Thought, polyencoders, average GloVe embeddings, and BERT embeddings.",
        "The proposed method outperforms BERT-MRC by +0.29 for English CoNLL2003, +0.96 for English OntoNotes5.0, +0.97 for Chinese MSRA, and +2.36 for Chinese OntoNotes4.0.",
        "Quora Duplicate Question Pair Detection and ranking questions in Bing's People Also Ask.",
        "They compared against various baselines including syntactic tree-based models, latent tree-based models, and non-tree models such as ELMo, Gumbel Tree-LSTM, Tree-based CNN, NSE, Reinforced Self-Attention Network, and Residual stacked encoders.",
        "Relation detection.",
        "A name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec).",
        "Browser-based annotation tool, manual categorization, and leveraging the structure of Flickr30K Entities with coreference annotations and Louvain clustering.",
        "English, French, Italian, Spanish, Hebrew, Arabic, German.",
        "They experimented with CAS-LSTMs, plain stacked LSTMs, and model variants including models with different forget gate values, models without the forget gate, and models with peephole connections.",
        "Yes.",
        "The authors experimented with ILP-based summarization and a few summarization algorithms provided by the Sumy package.",
        "The previous state of the art for this task was proposed by BIBREF0, but it had limitations, including requiring a hyperparameter for the number of latent states and not generalising well.",
        "The use of a 1-layer perceptron in the MP network.",
        "The corpus used for the task is a diachronic corpus pair from DTA corpus, consisting of subparts DTA18 and DTA19.",
        "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.",
        "The model achieves reasonable performance on target language reading comprehension, with some models achieving competitive performance compared to models trained on the target language.",
        "ALOHA achieves a significant improvement on the target character language style retrieval task compared to the baselines.",
        "ARAML performs better than several state-of-the-art GAN baselines with lower training variance.",
        "The authors present evidence through manual inspection of mislabeled items and examination of samples containing implicit abuse, as well as comparison with recent studies on biases in data collection and annotation.",
        "Yes.",
        "The dataset size is not explicitly mentioned in the article, but it is mentioned that the OurNepali dataset is almost ten times bigger in terms of entities compared to the ILPRL dataset.",
        "Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP.",
        "The datasets used include EEG data from BIBREF0, eye-tracking data, and self-paced reading time data, as well as a chapter of Harry Potter and the Sorcerer's Stone for magnetoencephalography (MEG) activity.",
        "unanswerable",
        "Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN.",
        "Traditional machine learning classifiers (Na\u00efve Bayes, Logistic Regression, Support Vector Machine, Random Forests, Gradient Boosted Trees) and neural network models (Convolutional Neural Networks, Recurrent Neural Networks, HybridCNN, RNN with Latent Topic Clustering).",
        "A bi-directional language model and a uni-directional language model, both using self-attention and transformer blocks.",
        "The weights are dynamically adjusted based on the $(1-p)$ factor, which changes as training proceeds.",
        "Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck of a score of 40 in Zork1, with KG-A2C-chained being more sample efficient and converging faster.",
        "A Bayesian model for each language.",
        "Annotations of the transcription include labels for mispronunciations, aborted words, and poor intelligibility.",
        "A semi-character RNN (ScRNN) is a word recognition model that processes a sentence of words with misspelled characters, predicting the correct words at each step.",
        "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish.",
        "NCEL consistently outperforms various baselines with a favorable generalization ability.",
        "yes",
        "The baseline used was the error detection system by Rei2016, trained on the same FCE dataset.",
        "The clinical notes were obtained from the CE task in 2010 i2b2/VA.",
        "It helps the refine decoder to focus on the context of the masked word.",
        "The article does not explicitly mention a specific dataset used for training the models, but it mentions that some models are trained on the book corpus, Paraphrase Database (PPDB), and Twitter.",
        "TF-IDF features.",
        "The dataset is annotated as no evidence of depression or evidence of depression, with further annotation of depressive symptoms if evidence of depression is present.",
        "The eight publicly available NER tasks used in the BioBERT paper.",
        "The training data was translated using the machine translation platform Apertium.",
        "They used a multinomial Naive Bayes classifier.",
        "A very simple logistic regression classifier with a single feature: the length of the sentence.",
        "They compare with a baseline model based on conditional random fields (CRF), and also with a pipeline approach where the classifier for pun detection is regarded as perfect.",
        "The political bias of different sources is included in the model by assigning a political bias label to different US outlets and then training the classifier on left-biased or right-biased networks and testing on the entire set of sources.",
        "The ancient Chinese data used in the study come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era, which were collected from the internet.",
        "English.",
        "standard benchmarks for English and Chinese.",
        "unanswerable",
        "The dataset used in this paper includes Flickr tags, structured scientific data, and various environmental datasets, such as the European network of nature protected sites Natura 2000 dataset, CORINE land cover classes, and SoilGrids.",
        "NUBes-PHI and MEDDOCAN.",
        "Unigrams, Unigram feature vectors, readability, word count, and linguistic artifacts.",
        "Avg. MCC, Avg. +ve F1 score, and Coverage are used to evaluate the predictive performance and strategy formulation ability of LiLi.",
        "Yes.",
        "Galatasaray and Fenerbah\u00e7e.",
        "The authors conduct experiments on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences.",
        "Gaussian-masked directional multi-head attention works by adjusting the weight between characters and their adjacent characters to a larger value, representing the effect of adjacent characters, and using a triangular matrix mask for forward and backward encoders to focus on different weights.",
        "Facebook and Twitter.",
        "The baseline features are the features extracted from the fully-connected layer of the baseline CNN, which have 100 neurons.",
        "The number of clusters (k) was varied in the experiments.",
        "Their system ranked second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard.",
        "The corpus consists of 53 documents, with an average of 156.1 sentences per document, and a total of 8,275 sentences and 167,739 words.",
        "unanswerable",
        "Text categorization and sentiment classification.",
        "Previous methods such as CNN, LSTM, and rule-based models, as well as other learned models like CPT2 and FDSF, are compared to the BERT-QC model.",
        "The newly produced ELMo embeddings were trained on corpora that are 10-14 times larger than the 20-million-token corpora used in the ELMoForManyLangs project.",
        "6946 sentences in the POS annotated dataset and 16225 sentences in the OurNepali dataset.",
        "MLP, Eusboost, MWMOTE.",
        "Yes.",
        "Yes.",
        "0.6103 in the 7th edition of BioASQ competition.",
        "The Wall Street Journal (WSJ) portion of the Penn Treebank.",
        "They mention that general-purpose deep learning frameworks like TensorFlow, PyTorch, and Keras offer huge flexibility but require a large overhead of mastering framework details.",
        "SimpleQuestions and WebQSP."
    ]
}
2026-01-11 14:02:14,623 | INFO : output_config file saved to /scratch/sj157/Small_World_Attention/scripts/small_world_inference/Llama-3.1-8B-Instruct/longbench/SmallWorld/Llama-3.1-8B-Instruct/LongAlpaca-12k/output_config.json.
2026-01-11 14:02:14,628 | INFO : Experiment longbench_LongAlpaca-12k_Llama-3.1-8B-Instruct_SmallWorld ended at 2026-01-11 14:02:14.620007-06:00. Duration: 0:18:57.081349
