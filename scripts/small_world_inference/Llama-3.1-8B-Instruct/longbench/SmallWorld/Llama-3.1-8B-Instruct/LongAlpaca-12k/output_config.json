{
    "pipeline_params": {
        "method": "smallworld",
        "kv_cache_method": "stream-llm",
        "model_name": "meta-llama/Llama-3.1-8B-Instruct",
        "save_path": "/savepath/for/model/checkpoints",
        "load_model_path": null,
        "train_mode": "inference_only",
        "n_init": 0.2,
        "n_local": 0.3,
        "only_eval": true,
        "resume": 0,
        "bf16": false,
        "batch_size_training": 1,
        "reset_optimizer": true,
        "lr": 0.0001,
        "weight_decay": 0.01,
        "gradient_accumulation_steps": 1,
        "num_epochs": 2,
        "eval_period": 1,
        "n_new_tokens": 64,
        "max_model_len": 128000,
        "random_walk_hadamard_dim": 128,
        "truncation_mode": "middle"
    },
    "eval_params": {
        "dataset": "qasper",
        "dataset_path": "dataset/longbench",
        "instruction": "You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nArticle: {context}\n\n Answer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: {input}\n\nAnswer:",
        "instruction_position": "prefix",
        "max_new_tokens": 128,
        "eval_metrics": [
            "qa_f1_score"
        ]
    },
    "train_params": {
        "dataset": "LongAlpaca-12k",
        "dataset_path": "Yukang/LongAlpaca-12k",
        "save_dir": "processed_dataset/LongAlpaca-12k-longctx/",
        "instruction": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:",
        "instruction_position": "prefix",
        "max_new_tokens": 128,
        "eval_metrics": [
            "qa_f1_score"
        ]
    },
    "eval_results": {
        "processed_results": {
            "dataset": "qasper",
            "score": 44.31709289550781,
            "outputs": [
                "The ground truth for fake news is established by a single person manually inspecting the text field within the tweets and labelling them as containing fake news or not, according to a characterization presented by Rubin et al.",
                "An extension to the NetVLAD approach, which adds Ghost clusters to map noisy or irrelevant content into ghost clusters.",
                "68.8% to 71.8%",
                "Context tweets, character-level features, and latent topic clustering (LTC) are proposed as additional features and context.",
                "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.",
                "yes",
                "Extrinsic evaluation is proposed for this task, specifically using the task of summary evaluation.",
                "The CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum.",
                "The proposed approach, GM_KL, outperforms existing approaches, w2g and w2gm, on benchmark word similarity and entailment datasets.",
                "Their ensemble method works by selecting the best performing models according to validation performance and then gradually adding the best performing models that have not been previously tried, discarding them if they do not improve validation performance.",
                "The Friends dataset comes from the scripts of the Friends TV sitcom, and the EmotionPush dataset is made up of Facebook messenger chats.",
                "English.",
                "The IMDb dataset of movie reviews.",
                "The proposed system (ALCrowd) achieves the best performance, significantly outperforming all other models with a p-value below 10^-5, and achieves +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ compared to the strong baseline LSTM-CRF.",
                "Yes.",
                "The Switchboard dataset, the Dialog State Tracking Challenge (DSTC) dataset, Twitter posts, and news articles are used.",
                "unanswerable",
                "RNN-based NMT and Transformer-NMT.",
                "A regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution.",
                "1. SVM with unigram, bigram, and trigram features, \n2. SVM with average word embedding, \n3. SVM with average transformed word embedding, \n4. CNN, \n5. Recurrent Convolutional Neural Networks (RCNN), \n6. UTCNN without user information, \n7. UTCNN without the LDA model, \n8. UTCNN without comments.",
                "They improved the state-of-the-art performance by several points, as shown in Table TABREF9.",
                "It eases interpretability and leads to crisper examples of attention head behavior, as well as novel behaviors unraveled thanks to the sparsity and adaptivity of the proposed model.",
                "The baseline model used was a Transformer base model, and a second baseline was the two-pass CADec model.",
                "Labeled Attachment Scores (LAS) and zero-shot accuracy for XNLI.",
                "MT.",
                "Emoticons, laughter expressions, and hashtag interpretations.",
                "A forward LSTM and a backwards LSTM.",
                "Yes.",
                "unanswerable",
                "22,880 users.",
                "BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-4, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), recipe-level coherence, and step entailment.",
                "They create labels such as \"Open-ended Inquiry\", \"Detailed Inquiry\", \"Multi-Intent Inquiry\", \"Reconfirmation Inquiry\", \"Inquiry with Transitional Clauses\", \"Yes/No Response\", \"Detailed Response\", \"Response with Revision\", and \"Response with Topic Drift\".",
                "unanswerable",
                "Machine translation tasks, including neural machine translation, are used for evaluation.",
                "The results for ELMo embeddings on the NER task show a significant improvement for Estonian, with the Macro F1 score being higher than the fastText baseline.",
                "The authors have a diversity of disciplinary backgrounds, including the humanities and social sciences.",
                "no \n\nThe paper uses LDA to extract features, but then uses these features in a supervised classification approach.",
                "The Nguni languages (zul, xho, nbl, ssw) and the Sotho languages (nso, sot, tsn) are similar to each other.",
                "conventional RNNs, bidirectional LSTM networks, and shallow LSTM networks.",
                "The Wikipedia dataset consists of around 29,794 articles, with the arXiv dataset consisting of three subsets of academic papers.",
                "A group of 50 native speakers who were well-versed in both English and Tamil languages acted as annotators for the evaluation.",
                "Yes.",
                "The models are evaluated based on the efficiency of a communication scheme (retention rate of tokens) and its accuracy (fraction of sentences generated by greedily decoding the model that exactly matches the target sentence).",
                "Precision, Recall, and F-measure are looked at for classification tasks.",
                "The source domain is an existing domain with sufficient labeled data, and the target domain is a new domain with very few or no labeled data.",
                "They compare with LSTMs and state-of-the-art methods such as RAN, QRNN, and NAS.",
                "Embedding Layer, Neural Network Layers, Loss Function, and Metrics, including word/character embedding, RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, attention mechanisms, regularization layers, and Focal Loss.",
                "The Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme for all experiments, and the corpus collected by deri2016grapheme for the multilingual task.",
                "unanswerable",
                "English, Spanish, Finnish, and 14 other languages.",
                "Named Entity Recognition, POS tagging, text classification, and language modeling.",
                "Yes.",
                "yes, it was evaluated against other encoder models and showed strong and robust performance in empirical evaluations on the response retrieval task.",
                "They utilize LIWC (Linguistic Inquiry and Word Count) and Meaning Extraction Method (MEM) to compile distributions for word categories that reflect psycholinguistic or semantic properties.",
                "claim, premise, backing, rebuttal, and refutation.",
                "n-grams of order INLINEFORM7 (which is not specified in the article)",
                "1,873 conversation threads, roughly 14k tweets.",
                "English, Mandarin Chinese, Spanish, French, Welsh, Kiswahili, Estonian, Finnish, Russian, Polish, Yue Chinese, and Hebrew.",
                "The Wikipedia dataset and the Reddit CMV dataset.",
                "unanswerable",
                "The quality of the data is empirically evaluated through various sanity checks, including sentence-level BLEU scores, perplexity, English character ratio, and similarity scores based on LASER cross-lingual sentence embeddings.",
                "They combine audio and text sequences in their RNN by concatenating the last hidden states of the audio-RNN and text-RNN and passing them through a feed-forward neural model.",
                "Our model improved by 2.11 BLEU, 1.7 FKGL, and 1.07 SARI compared to the baseline.",
                "7 annotators evaluated the results.",
                "A tweet is considered to have gone viral if it was retweeted more than 1000 times.",
                "unanswerable",
                "crowdsourcing via an Android application.",
                "Logistic Regression and a deep learning model adapted from Bowman et al. are used for RQE.",
                "The benchmark dataset is the social honeypot dataset created by Lee et al. lee2010devils, and its quality is not explicitly stated as high, but it is extensively explored in the paper.",
                "An LSTM decoder with an attention mechanism.",
                "no",
                "The best performing model among author's submissions is ensemble+ of (r4, r7, r12) for SLC task with a performance of 0.673 F1 on dev (external) and ensemble+ of (II, IV) for FLC task with a performance of 0.673 F1 on dev (external).",
                "The baseline was a weak NMT model without using any monolingual data, and a strong baseline established with monolingual data.",
                "0.7033.",
                "Word2vec, Skip\u2013gram, and CBOW are explored in the paper.",
                "They use a bilingual dictionary (Google Translate word translation) to translate each word in the source language into English.",
                "unanswerable",
                "Seven experts with legal training were recruited to construct answers to Turker questions.",
                "The CNN-RNN generative model is used for painting embedding, and the sequence-to-sequence model with global attention is used for language style transfer.",
                "The transformer layer works better, as evidenced by ToBERT outperforming RoBERT on the Fisher and 20newsgroups datasets.",
                "yes",
                "personal attack, racism, and sexism.",
                "They propose extended middle context, a new context representation for CNNs, by splitting the context into three disjoint regions: left context, middle context, and right context, and then combining the left context, left entity, and middle context, and the middle context, right entity, and right context.",
                "There are four major entity types (Person, Location, Organization, and Miscellaneous) and one additional type (post-positions) in the dataset.",
                "The expert annotations are higher quality than the crowd annotations.",
                "65% of speakers are men, speaking more than 75% of the time.",
                "The English-German dataset.",
                "BIBREF20, BIBREF18, and other recent neural models.",
                "Logistic Regression (LR) and Multilayer Perceptron (MLP) are used.",
                "NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, BIBREF17, BIBREF18, CogComp-NLP, and spaCy.",
                "The SQuAD dataset.",
                "Various approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries, but the usefulness of Flickr for characterizing the natural environment is less well-understood.",
                "Yes.",
                "The authors used CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus for evaluation.",
                "The IMDb movie review dataset.",
                "Yes.",
                "unanswerable",
                "The invertibility condition is that the neural projector's Jacobian determinant is nonzero and differentiable, and its inverse exists.",
                "The proposed qualitative annotation schema is a multi-label annotation schema that categorises gold standards according to linguistic complexity, required reasoning, and factual correctness, and further enriches qualitative annotations with a metric based on lexical cues.",
                "The WikiSmall dataset has 89,042 sentence pairs in the training set and 100 pairs in the test set, while the WikiLarge dataset has 296,402 sentence pairs in the training set and 359 pairs in the test set.",
                "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pretrain baseline, Triangle+pretrain baseline.",
                "English.",
                "The models used in the experiment are a linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model.",
                "unanswerable",
                "GloVe embeddings trained on 2 billion tweets and Edinburgh embeddings.",
                "All personalized models outperformed the baselines in BPE perplexity, and the Prior Name model performed the best.",
                "The harmonic mean of irony reward and sentiment reward.",
                "The generated English poem may not work well with Shakespeare style transfer when the style transfer dataset lacks similar words in the training sentences.",
                "The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset.",
                "The article presents the results of their distribution analysis in Table TABREF23, which reports the actual differences (together with their associated p-values) of the distributions of viral tweets containing fake news and viral tweets not containing fake news for every variable considered.",
                "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset.",
                "unanswerable",
                "A compact, scalable, and meaningful representation of a set of word vectors.",
                "The baseline model used for the article\u2013entity placement task is B1, which uses only salience-based features by Dunietz and Gillick.",
                "unanswerable",
                "unanswerable",
                "The SemEval-2016 \"Sentiment Analysis in Twitter\" challenge dataset.",
                "They use the pre-trained uncased BERT$_\\mathrm {BASE}$ model, which is smaller than BERT$_\\mathrm {LARGE}$.",
                "Yes, the article mentions that the authors carefully constructed baselines and performed close data inspection to ensure probe quality.",
                "Yes, the images are from the ShapeWorld framework, which is a controlled data generation framework consisting of abstract colored shapes.",
                "Our B-M model achieved competitive or even state-of-the-art results for some of the emotion labels on standard evaluation datasets.",
                "The tagging scheme employed is {INLINEFORM0, INLINEFORM1} for the simple tagging scheme and {INLINEFORM0, INLINEFORM2, INLINEFORM3} for the novel tagging scheme.",
                "unanswerable",
                "The model is considered robust if it can handle bias in prior knowledge and make accurate predictions despite unbalanced labeled features or class distributions.",
                "InferSent, Universal Sentence Encoder, Skip-Thought, GloVe, polyencoders, and average BERT embeddings.",
                "The proposed method outperforms BERT-MRC by +0.29 and +0.96 for English datasets, and achieves F1 improvements of +0.97 and +2.36 for Chinese datasets.",
                "Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask.",
                "They compared against syntactic tree-based models, latent tree-based models, and non-tree models, including ELMo, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, and Residual stacked encoders.",
                "Relation detection.",
                "A name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec).",
                "Browser-based annotation tool, manual categorization, tagging descriptions with part-of-speech information, and leveraging Flickr30K Entities with Louvain clustering.",
                "English, French, Italian, Spanish, German, Hebrew, Arabic, and Portuguese.",
                "They experimented with Cell-aware Stacked LSTMs (CAS-LSTMs), plain stacked LSTMs, and models with peephole connections.",
                "Yes.",
                "The authors experimented with ILP-based summarization and several summarization algorithms provided by the Sumy package.",
                "The previous state of the art for this task was proposed by BIBREF0, but its system and data are not available for replication.",
                "The skip connection of the master node.",
                "The diachronic corpus pair DTA18 and DTA19 from the DTA corpus.",
                "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.",
                "The model achieves competitive performance on target language reading comprehension, with fine-tuning on un-translated target language data achieving much better performance than data translated into the target language.",
                "ALOHA, combined with the HLAs and dialogue dataset, achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models.",
                "ARAML outperforms policy gradient in the stability of adversarial training and achieves better performance on three text generation tasks.",
                "The authors present evidence through manual inspection of mislabeled items and examining the content of tweets, which shows that the model can differentiate hate and offensive samples accurately by leveraging knowledge-aware language understanding, despite biases in the data.",
                "Yes.",
                "The dataset size is not explicitly mentioned in the article, however, it is mentioned that the OurNepal dataset volume is almost ten times bigger compared to ILPRL dataset in terms of entities.",
                "Using DSC loss improves F1 score by +0.58 for MRPC and +0.73 for QQP.",
                "The datasets used include data from BIBREF0, BIBREF2, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, and BIBREF9, as well as eye-tracking and self-paced reading data.",
                "unanswerable",
                "Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN.",
                "Traditional machine learning classifiers (Na\u00efve Bayes, Logistic Regression, Support Vector Machine, Random Forests, Gradient Boosted Trees) and neural network based models (Convolutional Neural Networks, Recurrent Neural Networks, HybridCNN, Latent Topic Clustering).",
                "A bi-directional language model and a uni-directional language model, both using self-attention and the Big Transformer architecture.",
                "The weights are dynamically adjusted in proportion to $(1-p)$, and this weight changes as training proceeds.",
                "Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck of a score of 40 in Zork1, with KG-A2C-chained being significantly more sample efficient and converging faster.",
                "A Bayesian model for each language.",
                "Annotations of non-standardized orthographic transcriptions include labels for mispronunciations, poor intelligibility, and undefined sound or pronunciations.",
                "A semi-character architecture is a type of RNN-based word recognition model that treats the first and last characters of a word individually and is agnostic to the ordering of internal characters.",
                "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish.",
                "NCEL outperforms state-of-the-art collective entity linking methods across five different datasets.",
                "yes",
                "The baseline used was the error detection system by Rei2016, trained on the same FCE dataset.",
                "The 2010 i2b2/VA dataset.",
                "It helps the refine decoder to feed a more complete input sequence to BERT, consistent with its pre-training process.",
                "The article does not explicitly mention a specific dataset used for training the models, but it mentions that some models are trained on the book corpus, PPDB (Paraphrase Database), and Twitter.",
                "TF-IDF features.",
                "The dataset is annotated as no evidence of depression (e.g., \"Citizens fear an economic depression\") or evidence of depression (e.g., \"depressed over disappointment\") with further annotation of depressive symptoms (e.g., depressed mood, disturbed sleep, fatigue or loss of energy).",
                "The eight publicly available NER tasks used in the BioBERT paper.",
                "The training data was translated using the machine translation platform Apertium.",
                "They used a multinomial Naive Bayes classifier.",
                "A very simple logistic regression classifier with default parameters for the SLC task, and a random technique generator for the FLC task.",
                "They compare with a baseline model based on conditional random fields (CRF), and a rule-based locator for heterographic puns.",
                "The political bias of different sources is included in the model by referring to the procedure described in BIBREF2, which labels different outlets as left-biased or right-biased, and by performing classification experiments on left-biased or right-biased networks separately.",
                "The ancient Chinese dataset comes from the internet, specifically from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.",
                "English.",
                "standard benchmarks for English and Chinese.",
                "unanswerable",
                "The dataset used in this paper includes Flickr tags, structured environmental datasets, and crowdsourced data from the ScenicOrNot website.",
                "NUBes-PHI and MEDDOCAN.",
                "Unigrams, Pragmatic features, Stylistic patterns, patterns related to situational disparity, Hastag interpretations, emoticons, and laughter expressions.",
                "Avg. MCC and avg. +ve F1 score.",
                "Yes.",
                "Galatasaray and Fenerbah\u00e7e.",
                "The article describes experiments on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences, including automatic evaluations and human evaluations.",
                "Gaussian-masked directional multi-head attention combines the Gaussian weight matrix with the standard scaled dot-product attention to adjust the weight between characters and their adjacent characters, making the relationship between adjacent characters more important.",
                "Facebook and Twitter.",
                "The baseline features are the features extracted from the fully-connected layer of the baseline CNN, which are 100 features.",
                "The number of clusters (k) was varied in the experiments.",
                "Their system ranked second (EI-Reg, EI-Oc), fourth (V-Reg), and fifth (V-Oc) on the SemEval AIT-2018 leaderboard.",
                "The corpus consists of 53 documents with an average of 156.1 sentences per document, totaling 8,275 sentences and 167,739 words.",
                "unanswerable",
                "Text categorization and sentiment classification.",
                "Previous methods such as term frequency models, CNN variants, and BERT models are compared to the proposed BERT-QC model.",
                "The newly produced ELMo embeddings were trained on corpora that are 10-14 times larger than the ones used in the ELMoForManyLangs project.",
                "6946 sentences",
                "MLP, Eusboost, MWMOTE.",
                "Yes.",
                "Yes.",
                "0.6103 in one of the test batches for Factoid Question Answering task.",
                "The Wall Street Journal (WSJ) portion of the Penn Treebank.",
                "They mention that general-purpose deep learning frameworks like TensorFlow, PyTorch, and Keras offer huge flexibility but require a large overhead of mastering framework details.",
                "SimpleQuestions and WebQSP."
            ]
        }
    },
    "management": {
        "exp_desc": "longbench_LongAlpaca-12k_Llama-3.1-8B-Instruct_SmallWorld",
        "pipeline_config_dir": "config/pipeline_config/SmallWorld/Llama-3.1-8B-Instruct/Llama-3.1-8B-Instruct-inference-32hadamard.json",
        "eval_config_dir": "config/eval_config/longbench/qasper.json",
        "output_folder_dir": "/savepath/for/dataset",
        "job_post_via": "slurm_sbatch",
        "slurm_info": {
            "slurm_job_id": "78314",
            "slurm_job_name": "trial",
            "slurm_out_file_dir": "/output/directory/for/slurm/job"
        },
        "sub_dir": {
            "input_config": "input_config/",
            "raw_results": "raw_results.json",
            "result_vis": "result_vis.png",
            "output_config": "output_config.json"
        },
        "start_time": "2026-01-14 08:38:19.500574-06:00",
        "end_time": "2026-01-14 08:43:14.081064-06:00",
        "exp_duration": "0:04:54.580490"
    }
}