{
    "pipeline_params": {
        "method": "smallworld",
        "kv_cache_method": "stream-llm",
        "model_name": "meta-llama/Llama-3.1-8B-Instruct",
        "save_path": "/scratch/sj157/checkpoints/Llama-3.2-1B-Instruct-selector",
        "load_model_path": null,
        "train_mode": "inference_only",
        "n_init": 0.2,
        "n_local": 0.3,
        "only_eval": true,
        "resume": 0,
        "bf16": false,
        "batch_size_training": 1,
        "reset_optimizer": true,
        "lr": 0.0001,
        "weight_decay": 0.01,
        "gradient_accumulation_steps": 1,
        "num_epochs": 2,
        "eval_period": 1,
        "n_new_tokens": 64,
        "max_model_len": 128000,
        "random_walk_hadamard_dim": 128,
        "truncation_mode": "middle"
    },
    "eval_params": {
        "dataset": "qasper",
        "dataset_path": "dataset/longbench",
        "instruction": "You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nArticle: {context}\n\n Answer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: {input}\n\nAnswer:",
        "instruction_position": "prefix",
        "max_new_tokens": 128,
        "eval_metrics": [
            "qa_f1_score"
        ]
    },
    "train_params": {
        "dataset": "LongAlpaca-12k",
        "dataset_path": "Yukang/LongAlpaca-12k",
        "save_dir": "processed_dataset/LongAlpaca-12k-longctx/",
        "instruction": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:",
        "instruction_position": "prefix",
        "max_new_tokens": 128,
        "eval_metrics": [
            "qa_f1_score"
        ]
    },
    "eval_results": {
        "processed_results": {
            "dataset": "qasper",
            "score": 23.344881057739258,
            "outputs": [
                "The ground truth for fake news is established by a human annotator who manually labeled the tweets as fake or not.",
                "An extension of the GMM-UBM model that uses utterance level features called i-vectors to classify a given audio signal into a particular class.",
                "68%",
                "Context-dependent characteristics, previous threads, user reporting processes, artificial intelligence, human reviewers, and othering language are proposed as additional features and context.",
                "Fox News, CNN, BBC News, The New York Times, The Guardian, The Washington Post, The Wall Street Journal, The Economist, The Times of London, The Financial Times, The Telegraph, The Independent, The Sun, The Daily Mail, The Daily Mirror, The Daily Star, The Daily Express, The Daily Record,",
                "yes",
                "Extrinsic evaluation is proposed for this task, but extrinsic evaluation is also used to test the quality of the generated summaries.",
                "The article mentions that the proposed approach is evaluated on three single-document news summarization datasets, which are representative of different writing conventions and summary styles.",
                "The approach is compared to other WSD approaches employing word embeddings, and is shown to outperform them in certain cases.",
                "Their ensemble method works by selecting the best performing model according to validation performance, then adding the best performing model that had not been previously tried model that had not been previously tried, and keeping it in the ensemble if it improves validation performance, and discarding otherwise",
                "The Friends dataset comes from the scripts of the Friends TV sitcom, and the EmotionPush dataset is made up of Facebook messenger chats.",
                "English.",
                "The English Wiki News Abstract and the English Wiki Simple (SW) Articles, and the Billion Word (BW) corpora are used for word embeddings, but the article does not specify which dataset is used for sentiment analysis.",
                "The proposed system outperforms strong baseline systems in Chinese NER tasks.",
                "Yes.",
                "The Switchboard dataset, the Switchboard dataset, the Switchboard dataset, the Switchboard, the Switchboard, the Switchboard dataset, the Switchboard, Switchboard, the Switchboard dataset, the Switchboard, the Switchboard dataset, the Switchboard, Switchboard, the Switchboard dataset, the Switchboard, the Switchboard, the Switchboard dataset, the Switchboard, the Switchboard, the Switchboard dataset, the Switchboard, the Switchboard, the Switchboard, the Switchboard, the Switchboard, the Switchboard, the Switchboard, the Switchboard, the Switchboard, the Switchboard, the",
                "unanswerable",
                "RSM models, SMT models, and a simple longest common subsequence based approach.",
                "A regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution..",
                "1. We propose UTCNN, a neural network for text in modern social media channels as well as legacy social media, forums, and message boards \u2014 anywhere that reveals users, their tastes, as well as their replies to posts. 2. When classifying social media post stances, we leverage users, including authors and likers. User embeddings can be generated even for users who have never posted anything. 3. We incorporate a topic model to automatically assign topics to each post in a single topic dataset. 4. We show that overall, the proposed method achieves the highest performance in all instances, and that all of the information",
                "They greatly improved the performance on the fine-grained sentiment classification task. is not explicitly stated in the article, however, the article does state that the approach mainly relied on the foundations of Sebastiani et al. work on multitask learning. work improved the performance by 5.5%",
                "It eases interpretability and leads to slight accuracy gains by allowing for exactly zero probabilities for irrelevant words.. also, it makes crisper examples of attention head behavior observable.",
                "The baseline was a context-agnostic MT system.",
                "L2 distance, masked language model objective, natural language inference, zero-shot crosslingual transfer tasks, and supervised dependency parsing task.",
                "MT, ASR, and ST respectively, but the attention module of ST benefits from the pre-training of MT.",
                "Emoticon and Pragmatic features.",
                "A single-layer LSTM architecture is used for the encoder, but it is also mentioned that a wide context window is used, encoding the entire available context.",
                "Yes.",
                "unanswerable",
                "22,000 blog users.",
                "BIBREF0, perplexity, user-ranking, and qualitative analysis are used for evaluation.",
                "They create labels for their dataset using a framework to construct a simulated human-human dialogue dataset. includes labels such as \"headaches\", \"the pain\", \"it\", \"head bulging\" all refer to the patient's headache symptom.",
                "unanswerable",
                "Machine translation and other complex tasks.",
                "The article does not provide a specific improvement in performance for Estonian in the NER task.",
                "The authors have a diversity of disciplinary backgrounds and research practices.",
                "no \n\nQuestion: What is the name of the method used to alluring social spammers' retweet in the paper?\n\nAnswer: social honeypot",
                "The N11 official languages of South Africa, and many of the South African languages, are still under-resourced.",
                "conventional fully-connected feed-forward deep neural networks.",
                "The Wikipedia dataset is novel to this paper, and the arXiv dataset is split into three sub-parts based on subject category, with no specific size mentioned.",
                "A group of    50 people, including native speakers and non-native speakers, were involved in the human judgements, but the exact number is not specified in the article.",
                "Yes., they test their framework performance on English-to-German translation, as well as under-resourced andEnglish-to-French and English-to-Spanish.",
                "The models are evaluated based on efficiency of typing keywords, accuracy of reconstruction, and interpretability of keywords., as well as typing time and variance of typing time.",
                "Precision, recall, F1 score, accuracy, and AUC.",
                "The source domain is an existing domain with sufficient labeled data, and the target domain is a new domain with very few or no labeled data.",
                "They compare with LSTMs and other recurrent units. also, they compare with the current state-of-the-art language model.",
                "Embeddings, recurrent neural networks, long short-term memory, transformers, convolutional neural networks, and feedforward networks are included in NeuronBlocks.",
                "The datasets used were the International Phonetic Alphabet (IPA) and Phoible and URIEL.",
                "unanswerable",
                "English, Spanish, and Basque, with additional languages including multiple languages in the multilingual corpora.",
                "Named entity recognition, tracking infectious diseases, and generation of hashtags not seen in the training dataset. is also mentioned as future work.",
                "Yes. They use pretrained embeddings, specifically the embeddings are from the word2vec model.",
                "yes, it was evaluated against traditional modular systems and more recent end-to-end task-oriented dialogue systems.",
                "They obtain psychological dimensions of people through analysis of their blog posts.",
                "claim, grounds, warrant, backing, qualifier, and rebuttal.",
                "n-grams",
                "1,400 posts from 1,000 users.",
                "English, Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese, German, Italian, Russian, Spanish, Arabic, Farsi, and Portuguese.",
                "The two datasets the model is applied to are the \"Wikipedia talk pages\" and \"Reddit comments\".",
                "unanswerable",
                "The quality of the data is empirically evaluated through several methods including augmenting transcripts with English translations, creating the largest ST corpus to date from TED talks, and using a multilingual speech corpus based on Bible readings.",
                "They combine audio andtext sequences in their RRNN by concatenating the outputs of two separate RNNs, one for audio and one for text.",
                "Our model achieved substantial gains in all tasks, and new SOTA results, with gains not specified in the article. however, the model improved by 2-5 fold in the article. however, the article does not specify the exact amount of improvement.",
                "7 humans evaluated the results.",
                "A tweet is considered to have gone viral if it has been retweeted or liked a certain number of times, specifically, the article does not specify the exact number.",
                "unanswerable",
                "crowdsourcing using an Android application.",
                "Logistic Regression, Recurrent Neural Networks (RNNs), Long Short Term Memory cells (LSTMs), and Convolutional Neural Networks (CNNs) are used for RQE.question similarity.question",
                "The benchmark dataset is the social honeypot built by alluring social spammers' retweet, but its quality is not mentioned. mentioned as being extensively explored in the paper.",
                "An LSTM-based architecture with attention.",
                "no",
                "The best performing model among the author's submissions is the model that achieved 3rd rank in FLC task with a performance that is not specified in the article.",
                "The baseline was a model trained on a resource-rich language pair like French English, which was fine-tuned for a low-resource language pair like Uzbek English.",
                "0.6103 for Factoid Question Answering task, and 0.6103 for List-type question answering task is not mentioned, however, their highest recall score in the fourth test batch set for List-type question answering task is mentioned.",
                "Word embedding techniques such as word2vec are explored in the paper.",
                "They use a parent task to fine-tune the learned parent model weights (features) for a related child task.",
                "unanswerable",
                "Seven experts were used for annotation, including Elias Wright, Gian Mascioli, Kiara Pillay, Harrison Kay, Eliel Talo, Alexander Fagella, and N. Cameron Russell..",
                "The CNN-RNN model is used for painting embedding, and the sequence-to-sequence model is used for language style transfer.",
                "The transformer layer works better, as it achieves state-of-the-art performance in several language understanding tasks.",
                "yes",
                "personalities such as racism, sexism, and gender.",
                "They propose extended middle context, a new context representation for CNNs, which uses all parts of the sentence and pays special attention to the middle part.",
                "There are four main categories of entities: \"PERSON\", \"LOCATION\", \"ORGANIZATION\" and others, with a total of four specific categories mentioned in the article.",
                "The resulting annotated data is of higher quality when a combination of expert and lay annotations is used compared to using lay data alone.",
                "65% of women are under-represented in analyzed corpora.",
                "The Multi30K dataset.",
                "BIBREF4, BIBREF13, BIBREF2, BIBREF15,",
                "Logistic regression is not mentioned, but logistic regression is not used, however, logistic regression is not used, but logistic regression is not used, however logistic regression is not used, however logistic regression is not used, but logistic regression is not used, however logistic regression is not used, but logistic regression is not used, however logistic regression is not used, but logistic regression is not used, however logistic regression is not used, but logistic regression is not used, however logistic regression is not used, but logistic regression is not used, however logistic regression is not used, but logistic regression is not used, however logistic regression is not used, but",
                "NLTK, Stanford CoreNLP, and CogComp-NLP are used.",
                "The SQuAD dataset and the SQuAD dataset is not mentioned in the text, however, the SQuAD dataset is not the dataset used in the experiments, the SQuAD dataset is not mentioned in the text, however the SQuAD dataset is not the dataset used in the experiments, the SQuAD dataset is not the the dataset used in the experiments, the SQuAD dataset is not the dataset used in the experiments, the SQuAD dataset is not the dataset used in the experiments, the SQuAD dataset is not the dataset used in the experiments, the SQuAD dataset is not the dataset used",
                "Various approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries.",
                "Yes.",
                "The authors used the following datasets for evaluation: topic identification of spoken conversations and call center customer satisfaction prediction, specifically using transcripts of calls that sometimes exceed 5000 words.",
                "The IMDb dataset is not mentioned, but the IMDb dataset is not used, the IMDb dataset is not mentioned in the article, however the IMDb dataset is not used, the IMDb dataset is not used, the IMDb dataset is not used, the IMDb dataset is not used, the IMDb dataset is not used, the IMDb dataset is not used, the IMDb dataset is not used, the IMDb dataset is not used, the IMDb dataset is not used, the IMDb dataset is not used, the IMDb dataset is not used, the IMDb dataset is not used, the IMDb dataset is not used,the IMDb dataset is not used, the IMDb dataset is",
                "Yes.",
                "unanswerable",
                "The invertibility condition is that the non-linear projector is chosen to be an invertible neural network, and the model is parameterized inthe inverse ofprojection.",
                "The proposed qualitative annotation schema is richly annotated with categories including linguistic complexity, reasoning, external knowledge, and factual correctness.",
                "The sizes of the two datasets are not explicitly mentioned in the article, but the article mentions that the MT-based approach has attracted great attention in the last several years, which addresses text simplification as a monolingual machine translation problem translating from 'ordinary' and'simplified' sentences.",
                "Van den Background ::: Problem Formulation, the baselines are end-to-end method, cascaded method, multi-task learning, pre-training techniques, and prior works.",
                "English.",
                "The models used in the experiment are Support Vector Machines (SVMs) and neural networks, specifically a CNN-based sentence classifier.",
                "unanswerable",
                "GloVe and Word2Vec were not mentioned, however, the article does mention that the pre-trained word embeddings were used, but it specifically mentions that the pre-trained word embeddings used were not specified.",
                "All three personalized models (Prior Tech, Prior Name, and Prior Recipe) generated high-quality and specific recipes that aligned with historical user preferences, as confirmed by both qualitative and quantitative analysis.",
                "The combination of rewards for reinforcement learning is the irony accuracy reward, the sentiment preservation reward, andthe content preservation reward, and the fluency reward.",
                "The authors demonstrate that limitation of their model in the absence of non-parallel datasets.",
                "The existing benchmarks they compared to include the large Hashtag Emotion Lexicon, Twitter, and distributed vector representations.",
                "The article does not provide a comprehensive list of their distribution results, but it mentions that they found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of users.",
                "The dataset of hashtags is sourced from tweets associated with tweets annotated in a multi-step process for higher quality than the previous dataset of 1,108 hashtags.",
                "unanswerable",
                "A word subspace can effectively and compactly represent the context of the corresponding text.",
                "The baseline model used is a supervised perceptron model.",
                "unanswerable",
                "unanswerable",
                "The dataset used was the tweets from the ternary and fine-grained sentiment classification settings, specifically the ones presented in Table TABREF3.",
                "They use small BERT, specifically the pre-trained BERT model.",
                "Yes, the article mentions that the authors \"carefully constructed baselines and close data inspection to ensure probe quality\".",
                "Yes, the images are from the ShapeWorld framework.",
                "Our models achieved state-of-the-art results on emotion detection, outperforming previous methods.",
                "The tagging scheme employed is a two-group categorization of English puns, namely heterographic puns and homographic puns.",
                "unanswerable",
                "The model is considered robust if it is insensitive to the prior knowledge and can handle situations where the prior knowledge may be biased or incomplete.",
                "InferSent, Universal Sentence Encoder, GloVe embeddings, and SentEval.",
                "The method improves F1 for the English OntoNotes5.0 dataset by 8 times and for the English CoNLL03 dataset by 5 times.",
                "Quora and passage retrieval tasks, include Quora and passagepassage retrieval.",
                "They compared against several baselines including recursive neural networks (RvNN), latent tree-structured models, and other models such as TG-RNN, TE-RNN/TE-RNTN, DC-TreeLSTMs, and typical LSTM.",
                "Relation detection.",
                "A simple encoder-decoder model and two other models (Prior Tech and Prior Recipe) are not baseline models, but the baseline models are actually the encoder-decoder model and two other models (Prior Tech and Prior Recipe) are not baseline models, the baseline models are actually the encoder-decoder model andPrior Tech and Prior Recipe are not baseline models, the baseline models are actually the encoder-decoder model and two other models are not baseline models, the baseline models are actually the encoder-decoder model and two other models are not baseline models, the baseline models are actually the encoder-decoder model and two other models are not baseline models, the",
                "Browsering through the article, the following methods are considered to find examples of biases and unwarrant inferences: \n\n1.  manually browsing through the Flickr30K dataset \n2. using a concrete example from the Flickr8K dataset \n3. manually examining the descriptions in the Flickr30K dataset.",
                "English, French, and other languages are not mentioned, but they explore languages in general.",
                "They experimented with stacked RNNs, LSTMs, and GRUs, and also with alternative methods that encourage direct and effective interaction between RNN layers by adding residual connections, shortcut connections, or using cell states of LSTMs. also a model that uses states from the left and lower context equally in computation of the new state.",
                "Yes.",
                "The authors experimented with an ILP-based summarization technique and compared it with manual summaries, but they also experimented with other algorithms such as logistic regression and sentence classification algorithms.",
                "The previous state of the art for instructor intervention in MOOC forums was proposed by earlier studies, which either modelled the entire context or required the context size to be specified explicitly.",
                "The READOUT pooling function is the least impactful component.",
                "The corpus used for the task is a digitized text resource, specifically the \"digitized text resources\" with the rise of large digitized text resources.",
                "Kannada, Telugu, Tamil, Marathi, Gujarati, Bengali, and Hindi are not mentioned in the article. however, the article does mention that the 7 Indian languages are not specified in the article but the languages are mentioned as Kannada, Telugu, Tamil, Marathi, Gujarati, Bengali, and Hindi are not mentioned in the article however, the article does mention that the 7 Indian languages are not specified in the article however, the article does mention that the 7 Indian languages are not specified in the article however, the article does mention that the 7 Indian languages are not specified in",
                "The model performance on target language reading comprehension is reported to surpass human-level performance on SQuAD, one of theRC benchmarks.",
                "ALOHA outperforms baselines, demonstrating state-of-the-art results on the ConvAI2 challenge.",
                "ARAML performs better than several state-of-the-art GAN baselines with lower training variance.",
                "The authors present evidence that the model can detect some biases in data annotation and collection by its ability to outperform previous works and its ability to detect biases in the process of collecting or annotating datasets.",
                "Yes.",
                "The dataset used for training the model is collected from various daily news sources from Nepal around the year 2015-2016, but the exact size of the dataset is not provided in the article.",
                "Using dice loss, the proposed training objective leads to a significant performance boost for paraphrase identification tasks, with F1 score improvements not specified.",
                "The article does not explicitly mention the datasets used, but it mentions that the results in Tables TABREF23, TABREF24, and TABREF25 reflect using only the forward-encoder, only the word embeddings, and the Pearson correlation coefficient between different measures, respectively.",
                "unanswerable",
                "Pointer to the best of our knowledge, the following baselines are used for evaluation: 1) To the best of our knowledge, we propose the the baseline, is used forPointer to the best of our knowledge, the following baselines are used for evaluation: 1) To the best of our knowledge, the following baselines are used for evaluation: 1) To the best of our knowledge, the following baselines are used for evaluation: 1) To the best of our knowledge, the following baselines are used for evaluation: 1) To the best of our knowledge, the following baselines are used for evaluation",
                "Traditional machine learning models are not used, but rather, the authors use bidirectional GRU networks with LTC.",
                "A generative model, a sequence to sequence model, and a transformer model are used.",
                "The weights are dynamically adjusted in proportion to (1-p), and this weight dynamically changes as training proceeds. is not entirely accurate, the weights are dynamically adjusted in proportion to (1-p) and this weight dynamically changes as training proceeds.",
                "Both proposed strategies, policy chaining and Go-Explore, surpass bottlenecks in text-adventure games. outperforming prior work.",
                "A Bayesian model for each language.",
                "Annotations or notes are used to identify non-standard pronunciation. is not mentioned in the article, is not mentioned. is not mentioned in the article.",
                "A semi-character architecture is a type of word recognition model that predicts each word in a sentence given a full sequence of possibly misspelled inputs.",
                "Bulgarian, Bulgarian, Dutch, English,French, German, Italian, Portuguese, Russian, Spanish, Swedish, and Turkish are explored, but the article specifically mentions that the languages explored are Bulgarian, Dutch, English, French, German, Italian, Portuguese Russian Spanish Swedish and Turkish are not mentioned in the the the article. the article specifically mentions that the languages explored are Bulgarian, Dutch, English, French German Italian Portuguese Russian Spanish Swedish and Turkish are not mentioned in the article but the article specifically mentions that the languages explored are Bulgarian Dutch English French German Italian Portuguese Russian Spanish Swedish and Turkish are not mentioned in the article but the article specifically",
                "NCEL outperforms the state-of-the-art collective methods across five different datasets.",
                "yes",
                "The baseline used was the performance of error detection algorithms without additional training data.",
                "The 2010 i2b2/VA BIBREF0 challenge.",
                "It allows the refine decoder to predict the refined word one-by-one using a pre-trained language model.",
                "The article does not specify a single dataset used, but mentions several datasets such as BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, and BIBREF7, which are likely references to various datasets used in the research. the article does not specify the names of these datasets. the article does not specify the names of these datasets.",
                "TF-IDF features are used.",
                "The dataset is annotated with labels for eating disorders, influenza, prescription drug and smoking behaviors, mental health disorders, and major depressive disorder.",
                "The eight biomedical NER tasks used for evaluation are not explicitly mentioned in the article. text, however, can be found in the reference section..",
                "The training data was translated using machine translation.",
                "They used a content-based classifier and an ensemble that incorporates several metadata classifiers.",
                "A feature-based system for discriminating between propagandist andarticles and",
                "They compare with systems developed separately for pun detection and pun location, as well as a system that makes a binary prediction on whether a sentence contains a pun or not.",
                "The authors consider a set of basic features which can be qualitatively interpreted w.r.t to the social behavior of users sharing credible vs non-credible information, and they also refer to BIBREF12 where authors successfully detect Twitter astroturfing content, i.e. political campaigns disguised as spontaneous grassroots, with a machine learning framework based on network features. also they use off-the-shelf machine learning classifiers to accurately classify news articles leveraging Twitter diffusion networks. also they use off-the-shelf Logistic Regression model on two different datasets of mainstream and disinformation news shared on Twitter respectively in the United States and in Italy during 2019.",
                "The ancient Chinese dataset comes from a large translation parallel corpus created by applying the proposed method to ancient texts.",
                "English.",
                "standard Chinese PTB.",
                "unanswerable",
                "The dataset used in this paper includes Flickr tags, structured environmental datasets (e.g. average temperature), and categorical datasets (e.g. land cover types).",
                "NUBes and MEDDOCAN: Medical Document Anonymization shared task dataset.",
                "Unigrams and Pragmatic features.",
                "Avg.predictive quality and strategy formulation ability.",
                "Yes., they employ their indexing-based method to create a silver-standard dataset for answer retrieval and triggering.",
                "Galatas and Fenerbah\u00e7e are unanswerable.",
                "The authors conduct experiments on irony generation, including building a large-scale dataset from Twitter, training a model to generate ironic sentences,preserving contents irrelevant to sentiment polarity and irony, andexploring the transformation from ironic sentences to non-ironic sentences, and comparing the performance of their model with other generative models.",
                "Gaussian-masked directional multi-head attention works by using a variant of self-attention that incorporates a Gaussian mask to capture localness and directional information.features.",
                "Facebook, Twitter, and Yelp, as well as general social media.the article",
                "The network's baseline features are the features extracted using the pre-trained sentiment, emotion, and personality models.",
                "The number of clusters was varied, the type of word vectors (skipgram, cbow, or GloVe) was also varied, and the dimensionality of the skipgram embeddings was also varied.",
                "Their system ranked second in EI-Reg, second in EI-Oc, and fourth in V-Reg.",
                "The corpus contains 10,000 clinical case reports.",
                "unanswerable",
                "Text categorization and sentiment classification.",
                "Previous methods include customized rule-based pattern matching, a combination of rule-based and machine learning approaches, CNN variants, LSTM variants, and simple term frequency models.. is also compared to methods that failed to generalize to medical domain questions. is also compared to methods that use one or a small number of generic solvers that perform little or no question decomposition.",
                "The training sets of these versions of ELMo are significantly larger, with the previous ones being monolingual corpora and these being seven morphologically rich, less-resourced languages.",
                "694 sentences in the OurNepali dataset and 694 sentences in thedataset.",
                "MLP, Eusboost, Mwmote, and Majority weighted minority oversampling technique.",
                "Yes.",
                "Yes.",
                "0.6103 in one of the test batches for Factoid Question Answering task.",
                "The Penn Treebank and the Penn Treebank.",
                "They mention that engineers often face challenges when applying DNN models to NLP tasks, which hinders their productivity.",
                "SimpleQuestions, WebQuestions, and both single-relation and multi-relation KBQA tasks."
            ]
        }
    },
    "management": {
        "exp_desc": "longbench_LongAlpaca-12k_Llama-3.1-8B-Instruct_SmallWorld",
        "pipeline_config_dir": "config/pipeline_config/SmallWorld/Llama-3.1-8B-Instruct/Llama-3.1-8B-Instruct-inference-32hadamard.json",
        "eval_config_dir": "config/eval_config/longbench/qasper.json",
        "output_folder_dir": "/scratch/sj157/Small_World_Attention/scripts/small_world_inference/Llama-3.1-8B-Instruct/longbench/SmallWorld/Llama-3.1-8B-Instruct/LongAlpaca-12k/",
        "job_post_via": "slurm_sbatch",
        "slurm_info": {
            "slurm_job_id": "76366",
            "slurm_job_name": "trial",
            "slurm_out_file_dir": "/scratch/sj157/slurm-76366.out"
        },
        "sub_dir": {
            "input_config": "input_config/",
            "raw_results": "raw_results.json",
            "result_vis": "result_vis.png",
            "output_config": "output_config.json"
        },
        "start_time": "2026-01-10 14:19:31.427551-06:00",
        "end_time": "2026-01-10 14:40:48.050771-06:00",
        "exp_duration": "0:21:16.623220"
    }
}